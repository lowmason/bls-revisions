{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9fb23bd7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "from typing import Iterable, List, Optional\n",
        "\n",
        "import httpx\n",
        "import polars as pl\n",
        "\n",
        "\n",
        "FRED_BASE = \"https://api.stlouisfed.org/fred\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9ac05cc7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _chunked(xs: List[str], n: int) -> Iterable[List[str]]:\n",
        "    for i in range(0, len(xs), n):\n",
        "        yield xs[i : i + n]\n",
        "\n",
        "\n",
        "def _request_with_retry(client: httpx.Client, url: str, params: dict,\n",
        "                        timeout: float = 30.0, max_retries: int = 6) -> httpx.Response:\n",
        "    \"\"\"GET with exponential backoff on 429 and transient 5xx errors.\"\"\"\n",
        "    import time as _time\n",
        "    for attempt in range(max_retries):\n",
        "        r = client.get(url, params=params, timeout=timeout)\n",
        "        if r.status_code == 429 or r.status_code >= 500:\n",
        "            wait = min(2 ** attempt, 60)\n",
        "            print(f\"    [{r.status_code}] retrying in {wait}s ...\")\n",
        "            _time.sleep(wait)\n",
        "            continue\n",
        "        r.raise_for_status()\n",
        "        return r\n",
        "    r.raise_for_status()\n",
        "    return r\n",
        "\n",
        "\n",
        "def get_vintage_dates(\n",
        "    client: httpx.Client,\n",
        "    series_id: str,\n",
        "    api_key: str,\n",
        ") -> List[str]:\n",
        "    r = _request_with_retry(\n",
        "        client,\n",
        "        f\"{FRED_BASE}/series/vintagedates\",\n",
        "        params={\n",
        "            \"series_id\": series_id,\n",
        "            \"api_key\": api_key,\n",
        "            \"file_type\": \"json\",\n",
        "        },\n",
        "        timeout=30.0,\n",
        "    )\n",
        "    data = r.json()\n",
        "    return data.get(\"vintage_dates\", [])\n",
        "\n",
        "\n",
        "def get_observations_for_vintages(\n",
        "    client: httpx.Client,\n",
        "    series_id: str,\n",
        "    api_key: str,\n",
        "    vintage_dates: List[str],\n",
        "    output_type: int = 2,\n",
        "    chunk_size: int = 200,\n",
        "    observation_start: Optional[str] = None,\n",
        ") -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Pull observations for a set of vintage_dates.\n",
        "\n",
        "    Notes:\n",
        "    - FRED allows comma-separated vintage_dates; chunking avoids query-length issues.\n",
        "    - output_type=2 returns wide-format with one column per vintage.\n",
        "    \"\"\"\n",
        "    frames: List[pl.DataFrame] = []\n",
        "\n",
        "    for chunk in _chunked(vintage_dates, chunk_size):\n",
        "        params = {\n",
        "            \"series_id\": series_id,\n",
        "            \"api_key\": api_key,\n",
        "            \"file_type\": \"json\",\n",
        "            \"vintage_dates\": \",\".join(chunk),\n",
        "            \"output_type\": str(output_type),\n",
        "        }\n",
        "        if observation_start is not None:\n",
        "            params[\"observation_start\"] = observation_start\n",
        "        r = _request_with_retry(\n",
        "            client,\n",
        "            f\"{FRED_BASE}/series/observations\",\n",
        "            params=params,\n",
        "            timeout=60.0,\n",
        "        )\n",
        "        obs = r.json().get(\"observations\", [])\n",
        "        if not obs:\n",
        "            continue\n",
        "\n",
        "        frames.append(pl.from_dicts(obs))\n",
        "\n",
        "    if not frames:\n",
        "        return pl.DataFrame(schema={\"date\": pl.Utf8})\n",
        "\n",
        "    df = pl.concat(frames, how=\"vertical_relaxed\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def compute_initial_and_latest_levels(df_wide: pl.DataFrame) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    From ALFRED output_type=2 wide-format observations, compute per date:\n",
        "      - initial_level: value from the earliest vintage\n",
        "      - latest_level:  value from the most recent vintage\n",
        "\n",
        "    Wide format has columns: date, {SERIES}_{YYYYMMDD}, ...\n",
        "    Vintage columns are ordered chronologically left-to-right.\n",
        "    \"\"\"\n",
        "    vintage_cols = [c for c in df_wide.columns if c != \"date\"]\n",
        "    if not vintage_cols:\n",
        "        return pl.DataFrame(schema={\"date\": pl.Date, \"initial_level\": pl.Float64, \"latest_level\": pl.Float64})\n",
        "\n",
        "    def _to_float(col_name: str) -> pl.Expr:\n",
        "        return (\n",
        "            pl.col(col_name)\n",
        "            .cast(pl.Utf8)\n",
        "            .str.replace_all(r\"^\\.$\", \"\")\n",
        "            .str.strip_chars()\n",
        "            .cast(pl.Float64, strict=False)\n",
        "        )\n",
        "\n",
        "    df = df_wide.with_columns(\n",
        "        pl.col(\"date\").str.to_date(strict=False).alias(\"date\"),\n",
        "        *[_to_float(c).alias(c) for c in vintage_cols],\n",
        "    ).filter(pl.col(\"date\").is_not_null())\n",
        "\n",
        "    df = df.with_columns(\n",
        "        pl.coalesce([pl.col(c) for c in vintage_cols]).alias(\"initial_level\"),\n",
        "        pl.coalesce([pl.col(c) for c in reversed(vintage_cols)]).alias(\"latest_level\"),\n",
        "    ).select(\"date\", \"initial_level\", \"latest_level\").sort(\"date\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def fetch_levels_initial_vs_latest(\n",
        "    series_id: str,\n",
        "    fred_api_key: Optional[str] = None,\n",
        "    last_n_vintages: Optional[int] = 24,\n",
        "    chunk_size: int = 200,\n",
        "    observation_start: Optional[str] = None,\n",
        ") -> pl.DataFrame:\n",
        "    api_key = fred_api_key or os.environ[\"FRED_API_KEY\"]\n",
        "\n",
        "    with httpx.Client() as client:\n",
        "        vintage_dates = get_vintage_dates(client, series_id=series_id, api_key=api_key)\n",
        "\n",
        "        if last_n_vintages is not None:\n",
        "            vintage_dates = vintage_dates[-last_n_vintages:]\n",
        "\n",
        "        df_obs = get_observations_for_vintages(\n",
        "            client,\n",
        "            series_id=series_id,\n",
        "            api_key=api_key,\n",
        "            vintage_dates=vintage_dates,\n",
        "            output_type=2,\n",
        "            chunk_size=chunk_size,\n",
        "            observation_start=observation_start,\n",
        "        )\n",
        "\n",
        "    return compute_initial_and_latest_levels(df_obs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5b09fe41",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "106 series |  53 states  |  1 industries\n"
          ]
        }
      ],
      "source": [
        "FIPS_TO_ABBREV = {\n",
        "    '01': 'AL', '02': 'AK', '04': 'AZ', '05': 'AR', '06': 'CA',\n",
        "    '08': 'CO', '09': 'CT', '10': 'DE', '11': 'DC', '12': 'FL',\n",
        "    '13': 'GA', '15': 'HI', '16': 'ID', '17': 'IL', '18': 'IN',\n",
        "    '19': 'IA', '20': 'KS', '21': 'KY', '22': 'LA', '23': 'ME',\n",
        "    '24': 'MD', '25': 'MA', '26': 'MI', '27': 'MN', '28': 'MS',\n",
        "    '29': 'MO', '30': 'MT', '31': 'NE', '32': 'NV', '33': 'NH',\n",
        "    '34': 'NJ', '35': 'NM', '36': 'NY', '37': 'NC', '38': 'ND',\n",
        "    '39': 'OH', '40': 'OK', '41': 'OR', '42': 'PA', '44': 'RI',\n",
        "    '45': 'SC', '46': 'SD', '47': 'TN', '48': 'TX', '49': 'UT',\n",
        "    '50': 'VT', '51': 'VA', '53': 'WA', '54': 'WV', '55': 'WI',\n",
        "    '56': 'WY', '72': 'PR', '78': 'VI',\n",
        "}\n",
        "\n",
        "# (ces_8digit_code, name, level, short_suffix_or_None)\n",
        "#\n",
        "# short_suffix: used for {ABBREV}{SUFFIX} IDs (~222 vintages from 2007)\n",
        "# None:         falls back to SMU{FIPS}00000{CES_8}01 (~139 vintages from 2014)\n",
        "INDUSTRIES = [\n",
        "    \n",
        "    # Domains\n",
        "    ('00000000', '00', 'Total Nonfarm', 'domain', 'NAN'),\n",
        "    ('05000000', '05', 'Total Private', 'domain', None),\n",
        "    ('06000000', '06', 'Goods-Producing', 'domain', None),\n",
        "    ('07000000', '07', 'Service-Providing', 'domain', None),\n",
        "    ('08000000', '08', 'Private Service-Providing', 'domain', None),\n",
        "    \n",
        "    # Supersectors\n",
        "    ('10000000', '10', 'Natural Resources and Mining', 'supersector', 'NRMNN'),\n",
        "    ('20000000', '20', 'Construction', 'supersector', 'CONSN'),\n",
        "    ('30000000', '30', 'Manufacturing', 'supersector', 'MFGN'),\n",
        "    ('40000000', '40', 'Trade, Transportation, and Utilities', 'supersector', 'TRADN'),\n",
        "    ('50000000', '50', 'Information', 'supersector', 'INFON'),\n",
        "    ('55000000', '55', 'Financial Activities', 'supersector', 'FIREN'),\n",
        "    ('60000000', '60', 'Professional and Business Services', 'supersector', 'PBSVN'),\n",
        "    ('65000000', '65', 'Education and Health Services', 'supersector', 'EDUHN'),\n",
        "    ('70000000', '70', 'Leisure and Hospitality', 'supersector', 'LEIHN'),\n",
        "    ('80000000', '80', 'Other Services', 'supersector', 'SRVON'),\n",
        "    ('90000000', '90', 'Government', 'supersector', 'GOVTN'),\n",
        "\n",
        "    # Sectors (SMU long-form only; some may not exist for every state)\n",
        "    ('10210000', '21', 'Mining', 'sector', None),\n",
        "    ('31000000', '31', 'Durable Goods', 'sector', None),\n",
        "    ('32000000', '32', 'Nondurable Goods', 'sector', None),\n",
        "    ('41000000', '41', 'Wholesale Trade', 'sector', None),\n",
        "    ('42000000', '42', 'Retail Trade', 'sector', None),\n",
        "    ('43000000', '43', 'Transp., Warehousing & Utilities', 'sector', None),\n",
        "    ('43220000', '22', 'Utilities', 'sector', None),\n",
        "    ('55520000', '52', 'Finance and Insurance', 'sector', None),\n",
        "    ('55530000', '53', 'Real Estate', 'sector', None),\n",
        "    ('60540000', '54', 'Prof., Scientific & Tech. Services', 'sector', None),\n",
        "    ('60550000', '55', 'Management of Companies', 'sector', None),\n",
        "    ('60560000', '56', 'Admin. and Support Services', 'sector', None),\n",
        "    ('65610000', '61', 'Private Educational Services', 'sector', None),\n",
        "    ('65620000', '62', 'Health Care and Social Assistance', 'sector', None),\n",
        "    ('70710000', '71', 'Arts, Entertainment & Recreation', 'sector', None),\n",
        "    ('70720000', '72', 'Accommodation and Food Services', 'sector', None),\n",
        "    ('90910000', '91', 'Federal Government', 'sector', None),\n",
        "    ('90920000', '92', 'State Government', 'sector', None),\n",
        "    ('90930000', '93', 'Local Government', 'sector', None),\n",
        "]\n",
        "\n",
        "def _make_series_id(fips: str, abbrev: str, ces_code: str,\n",
        "                    nsa_suffix: Optional[str], adjusted: bool) -> str:\n",
        "    if nsa_suffix is not None:\n",
        "        suffix = nsa_suffix[:-1] if adjusted else nsa_suffix\n",
        "        return f\"{abbrev}{suffix}\"\n",
        "    prefix = \"SMS\" if adjusted else \"SMU\"\n",
        "    return f\"{prefix}{fips}00000{ces_code}01\"\n",
        "\n",
        "\n",
        "rows = []\n",
        "for a in ['SA', 'NSA']:\n",
        "    adjusted = True if a == 'SA' else False\n",
        "    for fips, abbrev in FIPS_TO_ABBREV.items():\n",
        "        for ces_code, code, name, level, nsa_suffix in INDUSTRIES:\n",
        "            if code == '00':\n",
        "                rows.append({   \n",
        "                    'series_id': _make_series_id(fips, abbrev, ces_code, nsa_suffix, adjusted),\n",
        "                    'adjusted': adjusted,\n",
        "                    'geographic_type': 'state',\n",
        "                    'geographic_code': fips,\n",
        "                    'state_fips': fips,\n",
        "                    'state_abbrev': abbrev,\n",
        "                    'ces_industry': ces_code,\n",
        "                    'industry_type': level,\n",
        "                    'industry_code': code,\n",
        "                    'industry_name': name,\n",
        "                })\n",
        "\n",
        "series_df = pl.DataFrame(rows)\n",
        "print(\n",
        "    f\"{len(series_df)} series |  \"\n",
        "    f\"{series_df['state_fips'].n_unique()} states  |  \"\n",
        "    f\"{series_df['ces_industry'].n_unique()} industries\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4bad8b1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_batch_revisions(\n",
        "    series_df: pl.DataFrame,\n",
        "    fred_api_key: str,\n",
        "    last_n_vintages: int = 24,\n",
        "    chunk_size: int = 200,\n",
        "    sleep_between: float = 1.0,\n",
        "    observation_start: Optional[str] = None,\n",
        ") -> pl.DataFrame:\n",
        "    \"\"\"Fetch initial-vs-latest levels for every series in *series_df*.\n",
        "\n",
        "    Returns a long DataFrame with columns:\n",
        "        date, initial_level, latest_level,\n",
        "        series_id, state_fips, state_abbrev, ces_industry, industry_name\n",
        "    Series that don't exist on FRED (HTTP 400) are silently skipped.\n",
        "    \"\"\"\n",
        "    api_key = fred_api_key\n",
        "    results: list[pl.DataFrame] = []\n",
        "    skipped: list[str] = []\n",
        "    total = len(series_df)\n",
        "\n",
        "    with httpx.Client() as client:\n",
        "        for i, row in enumerate(series_df.iter_rows(named=True)):\n",
        "            sid = row[\"series_id\"]\n",
        "            try:\n",
        "                vdates = get_vintage_dates(client, series_id=sid, api_key=api_key)\n",
        "                if last_n_vintages is not None:\n",
        "                    vdates = vdates[-last_n_vintages:]\n",
        "                if not vdates:\n",
        "                    continue\n",
        "\n",
        "                df_obs = get_observations_for_vintages(\n",
        "                    client, series_id=sid, api_key=api_key,\n",
        "                    vintage_dates=vdates, output_type=2, chunk_size=chunk_size,\n",
        "                    observation_start=observation_start,\n",
        "                )\n",
        "                levels = compute_initial_and_latest_levels(df_obs)\n",
        "                if levels.height > 0:\n",
        "                    levels = levels.with_columns(\n",
        "                        pl.lit(sid).alias(\"series_id\"),\n",
        "                        pl.lit(row[\"adjusted\"]).alias(\"adjusted\"),\n",
        "                        pl.lit(row[\"state_fips\"]).alias(\"state_fips\"),\n",
        "                        pl.lit(row[\"state_abbrev\"]).alias(\"state_abbrev\"),\n",
        "                        pl.lit(row[\"ces_industry\"]).alias(\"ces_industry\"),\n",
        "                        pl.lit(row[\"geographic_type\"]).alias(\"geographic_type\"),\n",
        "                        pl.lit(row[\"geographic_code\"]).alias(\"geographic_code\"),\n",
        "                        pl.lit(row[\"industry_name\"]).alias(\"industry_name\"),\n",
        "                        pl.lit(row[\"industry_type\"]).alias(\"industry_type\"),\n",
        "                        pl.lit(row[\"industry_code\"]).alias(\"industry_code\"),\n",
        "                    )\n",
        "                    results.append(levels)\n",
        "\n",
        "                if (i + 1) % 25 == 0:\n",
        "                    print(f\"  [{i+1}/{total}] fetched {len(results)} series so far ({len(skipped)} skipped)\")\n",
        "\n",
        "            except httpx.HTTPStatusError as e:\n",
        "                if e.response.status_code in (400, 404):\n",
        "                    skipped.append(sid)\n",
        "                else:\n",
        "                    raise\n",
        "\n",
        "            time.sleep(sleep_between)\n",
        "\n",
        "    print(f\"\\nDone: {len(results)} series fetched, {len(skipped)} not found on FRED\")\n",
        "    if skipped:\n",
        "        print(f\"  Skipped (first 20): {skipped[:20]}\")\n",
        "\n",
        "    if not results:\n",
        "        return pl.DataFrame()\n",
        "        \n",
        "    return (\n",
        "        pl\n",
        "        .concat(\n",
        "            results, \n",
        "            how=\"vertical_relaxed\"\n",
        "        )\n",
        "        .select(\n",
        "            ref_date=pl.col('date')\n",
        "                       .dt.offset_by('11d'),\n",
        "            adjusted=pl.col('adjusted'),\n",
        "            geographic_type=pl.col('geographic_type'),\n",
        "            geographic_code=pl.col('geographic_code'),\n",
        "            industry_type=pl.col('industry_type'),\n",
        "            industry_code=pl.col('industry_code'),\n",
        "            employment_initial=pl.col('initial_level'),\n",
        "            employment_latest=pl.col('latest_level'),\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3b68fc16",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [25/106] fetched 25 series so far (0 skipped)\n",
            "  [50/106] fetched 50 series so far (0 skipped)\n",
            "  [75/106] fetched 73 series so far (2 skipped)\n",
            "  [100/106] fetched 98 series so far (2 skipped)\n",
            "\n",
            "Done: 102 series fetched, 4 not found on FRED\n",
            "  Skipped (first 20): ['PRNA', 'VINA', 'PRNAN', 'VINAN']\n"
          ]
        }
      ],
      "source": [
        "FRED_API_KEY = \"8d08f0f04f7d3e53fbdd765c0bbfb329\"\n",
        "OBS_START = \"2016-01-01\"\n",
        "\n",
        "sae_revisions = fetch_batch_revisions(\n",
        "    series_df, \n",
        "    fred_api_key=FRED_API_KEY, \n",
        "    observation_start=OBS_START,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0c62fe16",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of SAE NSA revision 0 observations:  6,120\n",
            "Number of SAE NSA revision 1 observations:  6,120\n"
          ]
        }
      ],
      "source": [
        "sae_revisions_nsa = (\n",
        "    sae_revisions\n",
        "    .filter(\n",
        "        pl.col('adjusted').eq(False)\n",
        "    )\n",
        ")\n",
        "\n",
        "sae_revisions_nsa_0 = (\n",
        "    sae_revisions_nsa\n",
        "    .rename({\n",
        "        'adjusted': 'seasonally_adjusted',\n",
        "        'employment_initial': 'employment'\n",
        "    })\n",
        "    .drop('employment_latest')\n",
        "    .with_columns(\n",
        "        revision=pl.lit(0, pl.UInt8)\n",
        "    )\n",
        "    .filter(\n",
        "        pl.col('employment').is_not_null()\n",
        "    )\n",
        ")\n",
        "\n",
        "sae_revisions_nsa_1 = (\n",
        "    sae_revisions_nsa\n",
        "    .rename({\n",
        "        'adjusted': 'seasonally_adjusted',\n",
        "        'employment_latest': 'employment'\n",
        "    })\n",
        "    .drop('employment_initial')\n",
        "    .with_columns(\n",
        "        revision=pl.lit(1, pl.UInt8)\n",
        "    )\n",
        "    .filter(\n",
        "        pl.col('employment').is_not_null()\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f'Number of SAE NSA revision 0 observations: {sae_revisions_nsa_0.height: ,}')\n",
        "print(f'Number of SAE NSA revision 1 observations: {sae_revisions_nsa_1.height: ,}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "aa5e1a9e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of SAE SA revision 0 observations:  6,120\n",
            "Number of SAE SA revision 1 observations:  6,120\n"
          ]
        }
      ],
      "source": [
        "sae_revisions_sa = (\n",
        "    sae_revisions\n",
        "    .filter(\n",
        "        pl.col('adjusted').eq(True)\n",
        "    )\n",
        ")\n",
        "\n",
        "sae_revisions_sa_0 = (\n",
        "    sae_revisions_sa\n",
        "    .rename({\n",
        "        'adjusted': 'seasonally_adjusted',\n",
        "        'employment_initial': 'employment'\n",
        "    })\n",
        "    .drop('employment_latest')\n",
        "    .with_columns(\n",
        "        revision=pl.lit(0, pl.UInt8)\n",
        "    )\n",
        "    .filter(\n",
        "        pl.col('employment').is_not_null()\n",
        "    )\n",
        ")\n",
        "\n",
        "sae_revisions_sa_1 = (\n",
        "    sae_revisions_sa\n",
        "    .rename({\n",
        "        'adjusted': 'seasonally_adjusted',\n",
        "        'employment_latest': 'employment'\n",
        "    })\n",
        "    .drop('employment_initial')\n",
        "    .with_columns(\n",
        "        revision=pl.lit(1, pl.UInt8)\n",
        "    )\n",
        "    .filter(\n",
        "        pl.col('employment').is_not_null()\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f'Number of SAE SA revision 0 observations: {sae_revisions_sa_0.height: ,}')\n",
        "print(f'Number of SAE SA revision 1 observations: {sae_revisions_sa_1.height: ,}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3b76ee76",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of SAE revision observations:  24,480\n"
          ]
        }
      ],
      "source": [
        "sae_revisions_1 = (\n",
        "    pl\n",
        "    .concat(\n",
        "        [\n",
        "            sae_revisions_nsa_0,\n",
        "            sae_revisions_nsa_1,\n",
        "            sae_revisions_sa_0,\n",
        "            sae_revisions_sa_1,\n",
        "        ]\n",
        "    )\n",
        "    .with_columns(\n",
        "        source=pl.lit('sae', pl.Utf8)\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f'Number of SAE revision observations: {sae_revisions_1.height: ,}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5d8bb019",
      "metadata": {},
      "outputs": [],
      "source": [
        "vintage_dates = (\n",
        "    pl\n",
        "    .read_parquet(\n",
        "        '/Users/lowell/Projects/bls-release-dates/data/vintage_dates.parquet'\n",
        "    )\n",
        "    .filter(\n",
        "        pl.col('publication').eq('sae')\n",
        "    )\n",
        "    .select(\n",
        "        ref_date=pl.col('ref_date'),\n",
        "        revision=pl.col('revision')\n",
        "                   .cast(pl.UInt8),\n",
        "        benchmark_revision=pl.col('benchmark_revision')\n",
        "                             .cast(pl.UInt8),\n",
        "        vintage_date=pl.col('vintage_date')\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "698cbb27",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of SAE revision observations (w/ dates):  24,174\n"
          ]
        }
      ],
      "source": [
        "sae_revisions_df = (\n",
        "    sae_revisions_1\n",
        "    .join(\n",
        "        vintage_dates,\n",
        "        on=['ref_date', 'revision'],\n",
        "        how='left'\n",
        "    )\n",
        "    .select(\n",
        "        'source',\n",
        "        'seasonally_adjusted',\n",
        "        'geographic_type', 'geographic_code', \n",
        "        'industry_type', 'industry_code', \n",
        "        'ref_date', 'vintage_date',\n",
        "        'revision', 'benchmark_revision', \n",
        "        'employment'\n",
        "    )\n",
        "    .filter(\n",
        "        pl.col('vintage_date').is_not_null()\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f'Number of SAE revision observations (w/ dates): {sae_revisions_df.height: ,}')\n",
        "\n",
        "#assert sae_revisions_df.height == sae_revisions_1.height"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7a971cf7",
      "metadata": {},
      "outputs": [],
      "source": [
        "sae_revisions_dups = (\n",
        "    sae_revisions_df\n",
        "    .unique(\n",
        "        subset=[\n",
        "            'source',\n",
        "            'seasonally_adjusted',\n",
        "            'geographic_type', 'geographic_code', \n",
        "            'industry_type', 'industry_code', \n",
        "            'ref_date', 'vintage_date',\n",
        "            'revision', 'benchmark_revision', \n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "assert sae_revisions_df.height == sae_revisions_dups.height"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "d94effdf",
      "metadata": {},
      "outputs": [],
      "source": [
        "(\n",
        "    sae_revisions_df\n",
        "    .write_parquet(\n",
        "        '/Users/lowell/Projects/bls-revisions/data/sae_revisions.parquet'\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de224652",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
